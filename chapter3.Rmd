---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---
```{r}
# extrafont::font_import(paths = NULL, recursive = TRUE, prompt = TRUE,pattern = NULL)
library(extrafont)
font_import(pattern = "D2")
```

```{r}
# 필요한 패키지를 불러옵니다.
library(tidyverse)
library(stringr)
library(stringi)
library(lubridate)
library(magrittr)

# 그래프 제목으로 자주 사용할 회사이름을 지정합니다. 
compNm <- 'afreecatv'

# RDS 파일을 읽습니다. 
dt <- readRDS(file = './Company_Review_Data_afreecatv.RDS')

names(dt) <- c("회사이름","회사코드","리뷰코드","직종구분","재직상태", "근무지역", "등록일자", "별점평가", '승진기회',  '복지급여', '워라밸', '사내문화', '경영진', '기업장점','기업단점', '바라는점','성장예상','추천여부')
```

```{r}
dt[, 8:13] <- sapply(X = dt[, 8:13], FUN = function(x) x / 20)

dt$`추천여부` <- str_extract(string = dt$`추천여부`, pattern = '이 기업을 추천 합니다(?=)')
dt$`추천여부`[is.na(x = dt$`추천여부`) == TRUE] <- '비추'
dt$`추천여부`[(dt$`추천여부` == '이 기업을 추천 합니다') == TRUE] <- '추천'


cols <- c('기업장점','기업단점', '바라는점','재직상태', '별점평가', '추천여부')
texts <- dt[, cols]

# delete NA
nrow(x = texts)
texts <- texts[complete.cases(texts), ]
nrow(x = texts)

# delete duplicated row
texts <- unique(x = texts)
nrow(x = texts)
```

```{r}
generateIDs <- function(obj, index = 'id') {

    if (obj %>% class() == 'data.frame') {
        n <- nrow(x = obj)
    } else {
        n <- length(x = obj)
    }
    
    id <- str_c(index, str_pad(
        string = 1:n,
        width = ceiling(x = log10(x = n)),
        side = 'left',
        pad = '0'
    ))
    
    return(id)
}

texts$id <- generateIDs(obj = texts, index = 'doc')

# 데이터를 붙여서 content 컬럼 생성
texts$content <- apply(X = texts[, c('기업장점', '기업단점', '바라는점')], MARGIN = 1, FUN = str_c, collapse = ' ')

# remove blanks
texts$content <- texts$content %>% str_remove_all(pattern = '\\s+')

texts <- texts[, c('id', 'content', '재직상태', '추천여부')]

texts <- unique(x = texts)

nrow(x = texts)
```

```{r}
textRange <- texts$content %>% nchar() %>% range()
print(x = textRange)

# 글자수 구간을 15개로 나눌 때 간격 계산
by <- ((textRange[2] - textRange[1]) / 15) %>% round(digits = -1L)
print(by)

# library(Hmisc)

cuts <- Hmisc::cut2(
    x = texts$content %>% nchar(),
    cuts = seq(from = 0, to = textRange[2], by = by),
    minmax = TRUE
)

freq <- table(cuts)
print(freq)

texts$content[nchar(x = texts$content) < 100]
```

```{r}
# NLP4kep 설치 : https://github.com/NamyounKim/NLP4kec
library(rJava)

# install.packages('D:\\Download\\NLP4kec_1.3.0.zip', repos = NULL)

library(NLP4kec)

parsed <- r_parser_r(contentVector = texts$content, language = 'ko')
length(x = parsed)

parsed[1:10]

# check 'NA' data
parsed[is.na(x = parsed) == TRUE]

# 글자수 확인
table(nchar(parsed))
# parsed %>% nchar() %>% table()
```
```{r}
library(tm)

corpus <- parsed %>% VectorSource() %>% VCorpus()
print(corpus)

str(corpus[[1]])

library(RWeka)

# make bigram
bigram <- function(x) {
    NGramTokenizer(x = x, control = Weka_control(min = 2, max = 2))
}

# make Term-Document Matrix
bigramList <- corpus %>% TermDocumentMatrix(control = list(tokenize = bigram)) %>% apply(MARGIN = 1, FUN = sum) %>% sort(decreasing = TRUE)

length(bigramList)

# 빈도수가 1% 이상 발생하는 bigram 만 남긴다.
bigramList <- bigramList[bigramList >= (nrow(x = texts)) * 0.01]

length(bigramList)

# bigram column 추출
bigramNames = names(bigramList)

# 최대 200개 까지만 확인
top <- if (length(x = bigramList) >= 100) bigramNames[1:100] else bigramNames
print(top)
```

```{r}

write.table(
    x = top,
    quote = FALSE,
    file = './spacing.txt',
    row.names = FALSE,
    col.names = FALSE
)

spacing <- read.table(file = './spacing.txt', sep = '\t')
colnames(x = spacing) <- 'before'

spacing <- unique(x = spacing)

spacing$after <- spacing$before %>% str_remove_all(pattern = ' ')

write.table(
    x = spacing$after,
    quote = FALSE,
    file = './dictionary.txt',
    row.names = FALSE,
    col.names = FALSE,
    fileEncoding = "utf-8"
)
```
```{r}
# 형태소 분석 (dictionary.txt 파일은 UTF-8 로 변환 후 사용해야 함.)
parsed <- r_parser_r(contentVector = texts$content, language = 'ko', korDicPath = './dictionary.txt')

# parsed <- r_parser_r(contentVector = texts$content, language = 'ko')

length(parsed)

parsed[1:10]
```
```{r}
# change corpus to vector source
corpus <- parsed %>% VectorSource() %>% VCorpus()

changeTerms <- function(corpus, before, after) {
    
    n <- length(x = corpus)
    
    for (i in 1:n) {
        corpus[[i]]$content <- corpus[[i]]$content %>% str_replace_all(pattern = as.character(before), replacement =  as.character(after))

    }
    
    return(corpus)
}


# 띄어쓰기 강제 적용
for (i in 1:nrow(x = spacing)) {
    corpus <- changeTerms(corpus = corpus, before = spacing$before[i], after = spacing$after[i])
}

# 의심되는 단어 변경여부 체크
checkTerms <- function(corpus, term) {
    corpus %>% sapply(FUN = `[[`, 'content') %>% str_detect(pattern = term) %>% sum() %>% print()
}
```

```{r}
checkTerms(corpus = corpus, term = '워라벨')
checkTerms(corpus = corpus, term = '일삶균형')
checkTerms(corpus = corpus, term = '워크라이프밸런스')
checkTerms(corpus = corpus, term = '자기계발기회')
checkTerms(corpus = corpus, term = '자기개발')
checkTerms(corpus = corpus, term = '네임벨류')
checkTerms(corpus = corpus, term = '군대 문화')
checkTerms(corpus = corpus, term = '군대식문화')
checkTerms(corpus = corpus, term = '수직문화')
checkTerms(corpus = corpus, term = '가정 날')
checkTerms(corpus = corpus, term = '손보')
checkTerms(corpus = corpus, term = '조직문화')
checkTerms(corpus = corpus, term = '회사문화')
checkTerms(corpus = corpus, term = '근로문화')
```
```{r}
corpus <- changeTerms(corpus = corpus, before = '조직문화', after = '기업문화')
```

```{r}
# corpus 객체 전처리 시 실수로 문자 벡터로 변환해 버릴 수 있음
# 반드시 corpus 객체속성 확인 필요 : PlainTextDocument
class(x = corpus[[1]])

# corpus 전처리 시 유용하게 사용 가능함
# 소문자 변경
corpus <- tm_map(x = corpus, FUN = content_transformer(FUN = tolower))

# 특수문자 제거
corpus <- tm_map(x = corpus, FUN = removePunctuation)

# 숫자 삭제
# corpus <- tm_map(x = corpus, FUN = removeNumbers)

myStopwords <- read.table(file = 'https://raw.githubusercontent.com/MrKevinNa/TextMining/master/stopwords.txt') %>% .$V1

# stopwords 제거
corpus <- tm_map(x = corpus, FUN = removeWords, myStopwords)

# whitespace 제거
corpus <- tm_map(x = corpus, FUN = stripWhitespace)

```
```{r}
# make data.frame
parsedDf <- data.frame(id = generateIDs(obj = parsed, index = 'doc'),
                       parsedContent = parsed,
                       corpusContent = sapply(X = corpus, FUN = `[[`, 'content'))

# make document-term matrix
dtm <- DocumentTermMatrix(x = corpus, control = list(wordLengths = c(2, Inf)))

# remove space (trim)
colnames(x = dtm) <- trimws(x = colnames(x = dtm), which = 'both')

# check demention
dim(x = dtm)

# optimizing

# dtm의 차원을 줄입니다. 
dtm <- removeSparseTerms(x = dtm, sparse = as.numeric(x = 0.99))

# 차원을 확인합니다.
dim(x = dtm)
```
```{r}
rowSums(x = dtm %>% as.matrix()) %>% table()

dtm$dimnames$Docs <- generateIDs(obj = dtm$dimnames$Docs, index = 'doc')

dtm$dimnames$Docs[1:40]

dtm$dimnames$Terms[1:40]
```

```{r}
# make tf-idf (Term Frequency - Inverse Document Frequency) matrix
# dtm <- DocumentTermMatrix(x = corpus, control = list(wordLengths = c(2, Inf)))
dtmTfIdf <- DocumentTermMatrix(x = corpus, control = list(
    removeNumbers = TRUE, # 숫자로 된 단어 제외
    wordLength = c(2, Inf),
    weighting = function(x) weightTfIdf(x, normalize = TRUE)
))

# remove space (trim)
# colnames(x = dtm) <- trimws(x = colnames(x = dtm), which = 'both')
colnames(x = dtmTfIdf) <- trimws(x = colnames(x = dtmTfIdf))

# check demention
# dim(x = dtm)
dim(dtmTfIdf)

dtmTfIdf <- removeSparseTerms(x =  dtmTfIdf, sparse = as.numeric(x = 0.99))

dim(x = dtmTfIdf)

rowSums(x = dtmTfIdf %>% as.matrix() %>% round(digits = 1L)) %>% table()

dtmTfIdf$dimnames$Docs <- generateIDs(obj = dtmTfIdf$dimnames$Docs, index = 'doc')
```
```{r}
# 시각화
wordsFreq <- dtm %>% as.matrix() %>% colSums()

length(x = wordsFreq)

# order by desc
wordsFreq <- wordsFreq[order(wordsFreq, decreasing = TRUE)]
head(x = wordsFreq, n = 20)

wordDf <- data.frame(word = names(x = wordsFreq), freq = wordsFreq, row.names = NULL) %>% arrange(desc(x = freq))

nrow(x = wordDf)

mytheme <- theme(
  plot.title = element_text(size = 14, face = 'bold', hjust = 0.5),
  axis.title.x = element_text(color = 'blue', size = 12, face = 'bold'),
  axis.title.y = element_text(color = '#993333', size = 12, face = 'bold'),
  axis.text.x = element_text(family = 'D2Coding', size = 10, face = 'bold'),
  axis.text.y = element_blank(), 
  axis.ticks.length = unit(0, 'cm'),
  panel.background = element_blank(),
  panel.grid = element_blank() )

ggplot(
  data = head(x = wordDf, n = 20L), 
  mapping = aes(
    x = reorder(word, -freq), 
    y = freq)) + 
  geom_bar(
    stat = 'identity', 
    fill = c(rep(x = 'gray30', times = 5), rep(x = 'gray80', times = 15))) +
  geom_text(
    mapping = aes(label = freq), 
    size = 4, 
    vjust = -0.5) + 
  labs(
    x = '고빈도 단어', 
    y = '빈도수', 
    title = '고빈도 단어 현황_전체') + 
  mytheme 
```

```{r}
# word-cloud
library(RColorBrewer)
library(wordcloud2)
library(htmlwidgets)

display.brewer.pal(n = 8, name = 'Set2')
```
```{r}
pal <- brewer.pal(n = 8, name = 'Set2')

if (nrow(x = wordDf) >= 300) wordCloud <- wordDf[1:300,] else wordCloud <- wordDf

wordcloud2(
  data = wordCloud,
  size = 0.8,
  fontFamily = 'D2Coding',
  color = pal,
  backgroundColor = 'white',
  minRotation = -pi / 4,
  maxRotation = pi / 4,
  shuffle = TRUE,
  rotateRatio = 0.25,
  shape = 'circle',
  ellipticity = 0.6)
```

```{r}
library(treemap)

treemap(
  dtf = wordDf,
  title = '고빈도 단어 트리맵',
  index = c('word'),
  vSize = 'freq',
  fontfamily.labels = 'D2Coding',
  fontsize.labels = 14,
  palette = pal,
  border.col = 'white')
```
```{r}
# 단어 연관성 분석

# dtmTfIdf 객체를 행렬로 변환 후 상관(계수) 행렬을 구한다.
corTerms <- dtmTfIdf %>% as.matrix() %>% cor()

dim(x = corTerms)

checkCorTerms <- function(n = 10, keyword) {
    
    
    corTerms %>% colnames() %>% str_subset(pattern = keyword) %>% print()

    
    corRef <- data.frame()
    
    # Error in corTerms[, keyword] : subscript out of bounds 오류 발생 -> 해당 케이스는 단어 연관분 분석 데이터 표기 불가
    corRef <- corTerms[, keyword] %>% sort(decreasing = TRUE) %>% data.frame() %>% set_colnames(c('corr'))
    
    head(x = corRef, n = n + 1)
}


# checkCorTerms(n = 10, keyword = '복지')
# checkCorTerms(n = 10, keyword = '자유')
# checkCorTerms(n = 10, keyword = '퇴근')
# checkCorTerms(n = 10, keyword = '개발')
# checkCorTerms(n = 10, keyword = '사람')
checkCorTerms(n = 10, keyword = '분위기')
# checkCorTerms(n = 10, keyword = '회사')
```

```{r}
# checkCorTerms(n = 10, keyword = '복지')
# checkCorTerms(n = 10, keyword = '자유')
# checkCorTerms(n = 10, keyword = '퇴근')
# checkCorTerms(n = 10, keyword = '개발')
# checkCorTerms(n = 10, keyword = '사람')
```


```{r}
# 상관 계수가 기준 이상인 단어들만 추출한다.
checkAssocs <- function(dtm, keyword, corr = 0.01) {
    
    # 재직 상태별 상관계수 생성
    createDtmObj <- function(dtm, workGb, n = 10) {
        
        # 재직여부 필터링
        dtmSmp <- dtm[rownames(x = dtm) %in% texts$id[texts$`재직상태` == workGb], ]
        
        # 상관 계수가 높은(0.01) 단어만 저장
        assocs <- findAssocs(x = dtmSmp, terms = keyword, corlimit = corr)
        
        # 재직여부 별 상관계수 
        dtmObj <- eval(expr = parse(text = str_c('assocs', keyword, sep = '$'))) %>% `[`(1:n) %>% as.data.frame() %>% set_colnames('corr')
        
        dtmObj$word <- rownames(x = dtmObj)
        
        rownames(x = dtmObj) <- NULL
        
        dtmObj$workGb <- workGb
        
        return(dtmObj)
    }
    
    dtmObj <- rbind(createDtmObj(dtm = dtm, workGb = '전직원'),
                    createDtmObj(dtm = dtm, workGb = '현직원'))
    
    plots <- lapply(X = split(x = dtmObj, f = dtmObj$workGb), FUN = function(x){
        
        # 상관계수 역순으로 변경
        x$word <- factor(x = x$word, levels = x$word[order(x$corr, decreasing = TRUE)])
        
        ggplot(data = x, 
           mapping = aes(
             x = word, 
             y = corr, 
             width = 0.8)) +
          geom_col(fill = 'gray50') +
          geom_text(mapping = aes(label = corr), 
                    size = 4, 
                    vjust = -0.5) + 
          scale_y_continuous(limits = c(0, max(x$corr)*1.1 )) + 
          labs(x = '', 
               y = '상관계수',
               title = str_c('[', unique(x$workGb), ']', 
                             keyword, 
                             '관련 연관성 높은 단어', 
                             sep = ' ')) + 
          theme(legend.position = 'none') + mytheme 
    }) # end of plots <- lapply(..)
    
    do.call(what = gridExtra::grid.arrange, args = c(plots, nrow = 2))
}
```

```{r}

# checkAssocs(dtm = dtmTfIdf, keyword = '복지')
checkAssocs(dtm = dtmTfIdf, keyword = '분위기')
```
```{r}
# make network map
library(network)
library(GGally)

drawNetworkmap <- function(dtmObj, title, sparse, corr, prob, link, cex) {
    
    corTerms <- dtmObj %>% as.matrix() %>% cor()
    corTerms[corTerms <= corr] <- 0
    
    netTerms <- network(x = corTerms, directed = FALSE)
    
    btnTerms <- sna::betweenness(dat = netTerms)
    netTerms %v% 'mode' <- ifelse(
        test = btnTerms >= quantile(x = btnTerms, probs = prob, na.rm = TRUE),
        yes = 'Top',
        no = 'Rest')
    
  nodeColors <- c('Top' = 'gold', 'Rest' = 'white')
  
  set.edge.value(x = netTerms, attrname = 'edgeSize', value = corTerms * 1.2)
  
  ggnet2(
    net = netTerms, # 네트워크 객체
    mode = 'fruchtermanreingold',
    layout.par = list(cell.jitter = 0.001),
    size.min = link,
    label = TRUE, # 노드에 라벨 표시 여부
    label.size = cex, # 라벨 폰트 사이즈
    node.color = 'mode',
    palette = nodeColors, # 노드 색상
    node.size = sna::degree(dat = netTerms),
    edge.size = 'edgeSize',
    legend.position = 'None',
    family = 'D2Coding') + 
    labs(title = title) + 
    theme(plot.title = element_text(hjust = 0.5, face = 'bold'))
}

dt4Networkmap1 <- function(workGb, sparse, corr, prob = 0.95, link = 4, cex = 4) {
    
    checks <- texts$`재직상태` == workGb
    dtmSub <- dtmTfIdf[rownames(x = dtmTfIdf) %in% texts$id[checks], ]
    
    dtmSub <- dtmSub[, as.matrix(x = dtmSub) %>% colSums() >= 1]
    
     title <- str_c('[', workGb, '] 네트워크맵', sep = ' ')
    
    # 네트워크 맵을 그립니다. 
    drawNetworkmap(dtmObj = dtmSub, title, sparse, corr, prob, link, cex)
}

dt4Networkmap1(workGb = '전직원', sparse = 0.95, corr = 0.30, link = 4, cex = 3)
```
```{r}
dt4Networkmap1(workGb = '전직원', sparse = 0.95, corr = 0.30, link = 2, cex = 3)
```

```{r}
dt4Networkmap1(workGb = '현직원', sparse = 0.95, corr = 0.30, link = 2, cex = 3)
```

```{r}
dt4Networkmap2 <- function(recomm, sparse, corr, prob = 0.95, link = 4, cex = 4) {
    
    checks <- texts$`추천여부` == recomm
    dtmSub <- dtmTfIdf[rownames(x = dtmTfIdf) %in% texts$id[checks], ]
    
    dtmSub <- dtmSub[, as.matrix(x = dtmSub) %>% colSums() >= 1]
    
     title <- str_c('[', recomm, '] 네트워크맵', sep = ' ')
    
    # 네트워크 맵을 그립니다. 
    drawNetworkmap(dtmObj = dtmSub, title, sparse, corr, prob, link, cex)
}

dt4Networkmap2(recomm = '추천', sparse = 0.90, corr = 0.30, link = 2, cex = 3)
```

```{r}
dt4Networkmap2(recomm = '비추', sparse = 0.90, corr = 0.30, link = 2, cex = 3)
```

